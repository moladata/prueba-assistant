AI Research Assistant - Astron Agent Workflow

This open-source workflow for Astron Agent transforms a simple topic into a comprehensive research report, complete with an executive summary. It's designed to automate the process of information gathering and synthesis, making it a powerful tool for students, analysts, and content creators.

This project was created as part of a sponsored video. You can watch the full tutorial and behind-the-scenes on Your YouTube Channel Name.


 <!-- It's highly recommended to add a GIF showing the workflow in action -->

‚ú® Features

‚Ä¢Automated Research: Generates a full report from a single topic.

‚Ä¢Multi-Stage Processing: Uses a 4-step LLM chain for deep analysis and structured output.

‚Ä¢Model Agnostic: Works with any OpenAI-compatible LLM (tested with Groq Llama 3.1 and Google Gemini).

‚Ä¢Easy to Use: Import and run the workflow in minutes.

‚Ä¢Fully Open Source: Free to use, modify, and distribute.

‚öôÔ∏è How It Works

The workflow is a sequential chain of four Large Language Model (LLM) nodes:

1.Generate Questions: Takes the user's topic and generates 5-7 key research questions.

2.Deep Analysis: Answers each question in detail, providing a comprehensive analysis.

3.Generate Report: Structures the analysis into a professional Markdown report with a title, table of contents, and organized sections.

4.Executive Summary: Creates a concise, 200-word summary of the final report, highlighting key findings and actions.

üöÄ Getting Started

Prerequisites

1.Docker and Docker Compose installed on your machine.

2.An OpenAI-compatible API Key from a provider like:

‚Ä¢Groq (Fast & Free)
‚Ä¢Google AI Studio (Free Tier)



1. Install Astron Agent

Clone the official Astron Agent repository and run it using Docker:

Bash


# Clone the repository
git clone https://github.com/iflytek/astron-agent.git

# Navigate to the docker directory
cd astron-agent/docker/astronAgent

# Create the .env file from the example
# On Windows (CMD ):
copy .env.example .env
# On Linux/macOS:
cp .env.example .env

# Start the services
docker compose -f docker-compose-with-auth.yaml up -d


Wait 3-5 minutes for all services to start. Access the interface at http://localhost/.

2. Configure the LLM

‚Ä¢Log in with admin / 123.

‚Ä¢Go to Model Management > + Create Model.

‚Ä¢Select Add Third Party Model.

‚Ä¢Fill in the details for your chosen LLM provider (e.g., Groq ):

‚Ä¢Model Name: Groq-Llama

‚Ä¢model: llama-3.1-8b-instant

‚Ä¢API Endpoint: https://api.groq.com/openai/v1

‚Ä¢API Key: Your Groq API Key



‚Ä¢Click Submit.

3. Import and Run the Workflow

1.Go to My Agents > + Create.

2.Click the Import button (top right ) and upload the AI-Research-Assistant.yml file from this repository.

3.The workflow will be imported. Click on it to open the orchestration view.

4.Click Debug (top right).

5.Enter a topic you want to research (e.g., "The future of renewable energy").

6.Click Run and watch the magic happen!

üìÑ Sample Output

Here is an example of a report generated by this workflow.

ü§ù Contributing

Contributions, issues, and feature requests are welcome! Feel free to check the issues page.

üìù License

This project is licensed under the MIT License - see the LICENSE file for details.




This project is for educational and demonstration purposes as part of a sponsored collaboration with iFlytek's Astron Agent.

